{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Using Reddit's API for Predicting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "In this project, we will practice two major skills. Collecting data via an API request and then building a binary predictor.\n",
    "\n",
    "As we discussed in week 2, and earlier today, there are two components to starting a data science problem: the problem statement, and acquiring the data.\n",
    "\n",
    "For this article, your problem statement will be: _What characteristics of a post on Reddit contribute most to the overall interaction (as measured by number of comments)?_\n",
    "\n",
    "Your method for acquiring the data will be scraping the 'hot' threads as listed on the [Reddit homepage](https://www.reddit.com/). You'll acquire _AT LEAST FOUR_ pieces of information about each thread:\n",
    "1. The title of the thread\n",
    "2. The subreddit that the thread corresponds to\n",
    "3. The length of time it has been up on Reddit\n",
    "4. The number of comments on the thread\n",
    "\n",
    "Once you've got the data, you will build a classification model that, using Natural Language Processing and any other relevant features, predicts whether or not a given Reddit post will have above or below the _median_ number of comments.\n",
    "\n",
    "**BONUS PROBLEMS**\n",
    "1. If creating a logistic regression, GridSearch Ridge and Lasso for this model and report the best hyperparameter values.\n",
    "1. Scrape the actual text of the threads using Selenium (you'll learn about this in Webscraping II).\n",
    "2. Write the actual article that you're pitching and turn it into a blog post that you host on your personal website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping Thread Info from Reddit.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a request (using requests) to the URL below. \n",
    "\n",
    "*NOTE*: Reddit will throw a [429 error](https://httpstatuses.com/429) when using the following code:\n",
    "```python\n",
    "res = requests.get(URL)\n",
    "```\n",
    "\n",
    "This is because Reddit has throttled python's default user agent. You'll need to set a custom `User-agent` to get your request to work.\n",
    "```python\n",
    "res = requests.get(URL, headers={'User-agent': 'YOUR NAME Bot 0.1'})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url_base = \"http://www.reddit.com/\"\n",
    "\n",
    "slug_hot = \"hot.json\"\n",
    "\n",
    "slug_subreddit = \"r/boston/\" # an optional intermediate slug to throw in to view a specific subreddit's 'hot' page\n",
    "\n",
    "user = {'User-agent': 'Jon Withers Bot 0.2'} # I need a User-agent to get in\n",
    "\n",
    "res = requests.get(url_base + slug_hot, headers = user) # put slug_subreddit in if you want\n",
    "\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the expected number of children: there should be 25 posts on Reddit's front page.\n",
    "\n",
    "Each of the 25 `children` have these attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `res.json()` to convert the response into a dictionary format and set this to a variable. \n",
    "\n",
    "```python\n",
    "data = res.json()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res.json()\n",
    "\n",
    "# all_keys = data['data']['children'][0]['data'].keys()\n",
    "# all_keys -= ['preview', 'post_hint']\n",
    "# things_to_get = list(all_keys)\n",
    "\n",
    "things_to_get = ['title','subreddit','created_utc','num_comments', 'score']\n",
    "\n",
    "list_of_dicts = []\n",
    "for i in range(25):\n",
    "    this_dict = {}\n",
    "    for j in range(len(things_to_get)):\n",
    "\n",
    "        this_key = things_to_get[j]\n",
    "        this_dict[this_key] = data['data']['children'][i]['data'][this_key]\n",
    "    list_of_dicts.append(this_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.527616e+09</td>\n",
       "      <td>5167</td>\n",
       "      <td>15689</td>\n",
       "      <td>television</td>\n",
       "      <td>‘Roseanne’ Cancelled: ABC Scraps Season 11 Aft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.527616e+09</td>\n",
       "      <td>3252</td>\n",
       "      <td>13337</td>\n",
       "      <td>news</td>\n",
       "      <td>ABC cancels Roseanne Barr's sitcom after her t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.527609e+09</td>\n",
       "      <td>1470</td>\n",
       "      <td>32992</td>\n",
       "      <td>creepy</td>\n",
       "      <td>A friend went for a walk the other night and s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.527614e+09</td>\n",
       "      <td>510</td>\n",
       "      <td>9880</td>\n",
       "      <td>mildlyinfuriating</td>\n",
       "      <td>North Face stole my photo and put it on their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.527608e+09</td>\n",
       "      <td>922</td>\n",
       "      <td>43964</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>Advanced Pettiness™️</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.527607e+09</td>\n",
       "      <td>1894</td>\n",
       "      <td>19651</td>\n",
       "      <td>videos</td>\n",
       "      <td>Millionaire televangelists Kenneth Copeland an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.527606e+09</td>\n",
       "      <td>712</td>\n",
       "      <td>22215</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>Please Don't Roast Marshmallows Over the Erupt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.527607e+09</td>\n",
       "      <td>415</td>\n",
       "      <td>16015</td>\n",
       "      <td>funny</td>\n",
       "      <td>Apologies Accepted.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.527604e+09</td>\n",
       "      <td>1297</td>\n",
       "      <td>43171</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>TIL cartographers protect their intellectual p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.527605e+09</td>\n",
       "      <td>240</td>\n",
       "      <td>17602</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>Since its lemonade stand season, lets apprecia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.527604e+09</td>\n",
       "      <td>1209</td>\n",
       "      <td>34062</td>\n",
       "      <td>gaming</td>\n",
       "      <td>Mod it until it's visually pleasing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.527602e+09</td>\n",
       "      <td>136</td>\n",
       "      <td>21917</td>\n",
       "      <td>BikiniBottomTwitter</td>\n",
       "      <td>Back in my day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.527602e+09</td>\n",
       "      <td>367</td>\n",
       "      <td>18788</td>\n",
       "      <td>lego</td>\n",
       "      <td>My grandpa is quickly loosing brain function a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.527599e+09</td>\n",
       "      <td>245</td>\n",
       "      <td>53395</td>\n",
       "      <td>aww</td>\n",
       "      <td>His wiggle was too strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.527600e+09</td>\n",
       "      <td>2083</td>\n",
       "      <td>26540</td>\n",
       "      <td>Whatcouldgowrong</td>\n",
       "      <td>Just punch this guy in the face wcgw.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.527603e+09</td>\n",
       "      <td>292</td>\n",
       "      <td>14181</td>\n",
       "      <td>IdiotsInCars</td>\n",
       "      <td>Mods are asleep, upvote car inside idiot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.527602e+09</td>\n",
       "      <td>263</td>\n",
       "      <td>12837</td>\n",
       "      <td>OldSchoolCool</td>\n",
       "      <td>David Bowie on the set of 'Good Morning Americ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.527612e+09</td>\n",
       "      <td>179</td>\n",
       "      <td>5181</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>At least she's being honest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.527599e+09</td>\n",
       "      <td>593</td>\n",
       "      <td>22044</td>\n",
       "      <td>gifs</td>\n",
       "      <td>Skittles chroma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.527596e+09</td>\n",
       "      <td>1919</td>\n",
       "      <td>46354</td>\n",
       "      <td>Wellthatsucks</td>\n",
       "      <td>New skelton found in Pompeii: this guy was run...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.527597e+09</td>\n",
       "      <td>176</td>\n",
       "      <td>28252</td>\n",
       "      <td>oddlysatisfying</td>\n",
       "      <td>The snow on this fence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.527605e+09</td>\n",
       "      <td>99</td>\n",
       "      <td>7123</td>\n",
       "      <td>photoshopbattles</td>\n",
       "      <td>PsBattle: Woman trying to walk her cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.527597e+09</td>\n",
       "      <td>202</td>\n",
       "      <td>20058</td>\n",
       "      <td>IASIP</td>\n",
       "      <td>Finally got Monster Hunter. I'm going for GASPS!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.527603e+09</td>\n",
       "      <td>338</td>\n",
       "      <td>7491</td>\n",
       "      <td>nostalgia</td>\n",
       "      <td>Working on the car with your dad so you can le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.527603e+09</td>\n",
       "      <td>126</td>\n",
       "      <td>8998</td>\n",
       "      <td>justneckbeardthings</td>\n",
       "      <td>Flies can be neckbeards, too</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     created_utc  num_comments  score            subreddit  \\\n",
       "0   1.527616e+09          5167  15689           television   \n",
       "1   1.527616e+09          3252  13337                 news   \n",
       "2   1.527609e+09          1470  32992               creepy   \n",
       "3   1.527614e+09           510   9880    mildlyinfuriating   \n",
       "4   1.527608e+09           922  43964   BlackPeopleTwitter   \n",
       "5   1.527607e+09          1894  19651               videos   \n",
       "6   1.527606e+09           712  22215          nottheonion   \n",
       "7   1.527607e+09           415  16015                funny   \n",
       "8   1.527604e+09          1297  43171        todayilearned   \n",
       "9   1.527605e+09           240  17602        DunderMifflin   \n",
       "10  1.527604e+09          1209  34062               gaming   \n",
       "11  1.527602e+09           136  21917  BikiniBottomTwitter   \n",
       "12  1.527602e+09           367  18788                 lego   \n",
       "13  1.527599e+09           245  53395                  aww   \n",
       "14  1.527600e+09          2083  26540     Whatcouldgowrong   \n",
       "15  1.527603e+09           292  14181         IdiotsInCars   \n",
       "16  1.527602e+09           263  12837        OldSchoolCool   \n",
       "17  1.527612e+09           179   5181             facepalm   \n",
       "18  1.527599e+09           593  22044                 gifs   \n",
       "19  1.527596e+09          1919  46354        Wellthatsucks   \n",
       "20  1.527597e+09           176  28252      oddlysatisfying   \n",
       "21  1.527605e+09            99   7123     photoshopbattles   \n",
       "22  1.527597e+09           202  20058                IASIP   \n",
       "23  1.527603e+09           338   7491            nostalgia   \n",
       "24  1.527603e+09           126   8998  justneckbeardthings   \n",
       "\n",
       "                                                title  \n",
       "0   ‘Roseanne’ Cancelled: ABC Scraps Season 11 Aft...  \n",
       "1   ABC cancels Roseanne Barr's sitcom after her t...  \n",
       "2   A friend went for a walk the other night and s...  \n",
       "3   North Face stole my photo and put it on their ...  \n",
       "4                                Advanced Pettiness™️  \n",
       "5   Millionaire televangelists Kenneth Copeland an...  \n",
       "6   Please Don't Roast Marshmallows Over the Erupt...  \n",
       "7                                 Apologies Accepted.  \n",
       "8   TIL cartographers protect their intellectual p...  \n",
       "9   Since its lemonade stand season, lets apprecia...  \n",
       "10               Mod it until it's visually pleasing.  \n",
       "11                                     Back in my day  \n",
       "12  My grandpa is quickly loosing brain function a...  \n",
       "13                          His wiggle was too strong  \n",
       "14              Just punch this guy in the face wcgw.  \n",
       "15           Mods are asleep, upvote car inside idiot  \n",
       "16  David Bowie on the set of 'Good Morning Americ...  \n",
       "17                        At least she's being honest  \n",
       "18                                    Skittles chroma  \n",
       "19  New skelton found in Pompeii: this guy was run...  \n",
       "20                            The snow on this fence.  \n",
       "21             PsBattle: Woman trying to walk her cat  \n",
       "22   Finally got Monster Hunter. I'm going for GASPS!  \n",
       "23  Working on the car with your dad so you can le...  \n",
       "24                       Flies can be neckbeards, too  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list_of_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting more results\n",
    "\n",
    "By default, Reddit will give you the top 25 posts:\n",
    "\n",
    "```python\n",
    "print(len(data['data']['children']))\n",
    "```\n",
    "\n",
    "If you want more, you'll need to do two things:\n",
    "1. Get the name of the last post: `data['data']['after']`\n",
    "2. Use that name to hit the following url: `http://www.reddit.com/hot.json?after=THE_AFTER_FROM_STEP_1`\n",
    "3. Create a loop to repeat steps 1 and 2 until you have a sufficient number of posts. \n",
    "\n",
    "*NOTE*: Reddit will limit the number of requests per second you're allowed to make. When you create your loop, be sure to add the following after each iteration.\n",
    "\n",
    "```python\n",
    "time.sleep(3) # sleeps 3 seconds before continuing```\n",
    "\n",
    "This will throttle your loop and keep you within Reddit's guidelines. You'll need to import the `time` library for this to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1527278136.044412"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_dicts = []\n",
    "import time\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = time.time()\n",
    "for k in range(40):\n",
    "    time.sleep(3)\n",
    "    if k == 0:\n",
    "        res = requests.get(url_base + slug_hot, user)\n",
    "    else:\n",
    "        res = requests.get(url_base + slug_hot + '?after=' + aft, user)\n",
    "    \n",
    "    things_to_get = list(all_keys)\n",
    "\n",
    "    for i in range(25):\n",
    "        this_dict = {}\n",
    "        for j in range(len(things_to_get)):\n",
    "\n",
    "            this_key = things_to_get[j]\n",
    "            this_dict[this_key] = data['data']['children'][i]['data'][this_key]\n",
    "        list_of_dicts.append(this_dict)\n",
    "    aft = data['data']['after']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_since_creation'] = current_time - df['created_utc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.dropna(how='all', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.dropna(thresh=(0.5* 1025), axis = 1)\n",
    "len(df.dropna(thresh=(0.5 * 1025), axis = 1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123, 123]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df2.isna().sum()[df.isna().sum()!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('scraped_friday.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Texas Republican Who Pushed To Impeach Obama Just Got Jailed After Being Convicted Of 23 Felonies'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[7,'title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Collect more information\n",
    "\n",
    "While we only require you to collect four features, there may be other info that you can find on the results page that might be useful. Feel free to write more functions so that you have more interesting and useful data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV\n",
    "You may do this regularly while scraping data as well, so that if your scraper stops of your computer crashes, you don't lose all your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "783fd153-28ac-47ab-bfca-27e7c1de95b4"
   },
   "outputs": [],
   "source": [
    "# Export to csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "## Predicting comments using Random Forests + Another Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "243e949e-2742-40af-872e-fec475fd306c"
   },
   "source": [
    "#### Load in the the data of scraped results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "#### We want to predict a binary variable - whether the number of comments was low or high. Compute the median number of comments and create a new binary variable that is true when the number of comments is high (above the median)\n",
    "\n",
    "We could also perform Linear Regression (or any regression) to predict the number of comments here. Instead, we are going to convert this into a _binary_ classification problem, by predicting two classes, HIGH vs LOW number of comments.\n",
    "\n",
    "While performing regression may be better, performing classification may help remove some of the noise of the extremely popular threads. We don't _have_ to choose the `median` as the splitting point - we could also split on the 75th percentile or any other reasonable breaking point.\n",
    "\n",
    "In fact, the ideal scenario may be to predict many levels of comment numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "#### Create a Random Forest model to predict High/Low number of comments using Sklearn. Start by ONLY using the subreddit as a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "ddbc6159-6854-4ca7-857f-bfecdaf6d9c2"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "0ef04f32-419c-4bf2-baf7-48201f03df89"
   },
   "source": [
    "#### Create a few new variables in your dataframe to represent interesting features of a thread title.\n",
    "- For example, create a feature that represents whether 'cat' is in the title or whether 'funny' is in the title. \n",
    "- Then build a new Random Forest with these features. Do they add any value?\n",
    "- After creating these variables, use count-vectorizer to create features based on the words in the thread titles.\n",
    "- Build a new random forest model with subreddit and these new features included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy of the model, as well as any other metrics you feel are appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "269b9e7c-60b5-4a06-8255-881d7395bc1b"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the model-building process with a non-tree-based method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "#### Use Count Vectorizer from scikit-learn to create features from the thread titles. \n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "---\n",
    "Put your executive summary in a Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "3be94357-e551-4094-b784-2df039216d33"
   },
   "source": [
    "### BONUS\n",
    "Refer to the README for the bonus parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "4239e458-28bd-4675-8db3-c1d9c02b9854"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
