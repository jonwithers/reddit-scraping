{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the urls for requests\n",
    "The reddit data can be accessed for each subreddit using the following format: `http://www.reddit.com/r/{subreddit}/hot.json`. Reddit doesn't allow the default Python user so I have to set my own (here, `JonBot 0.1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = \"http://www.reddit.com/\"\n",
    "\n",
    "slug_hot = \"hot.json\"\n",
    "\n",
    "slug_bpt = \"r/BlackPeopleTwitter/\" # an optional intermediate slug to throw in to view a specific subreddit's 'hot' page\n",
    "slug_wpt = \"r/WhitePeopleTwitter/\"\n",
    "\n",
    "user = {'User-agent': 'JonBot 0.1'} # I need a User-agent to get in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping r/BlackPeopleTwitter\n",
    "The script below sets up an empty dictionary and chooses what extra features to extract from each top-level comment. It then scrapes the number of comments, post title, top-level comment, upvotes for that comment, and the time that comment was created for `n_scrapes` posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-40c1d3eea25c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mthis_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'current_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#     print(comment_data['ups'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mlist_of_dictionaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_of_dictionaries = []\n",
    "aft = ''\n",
    "features = ['created_utc', 'body', 'ups']\n",
    "n_scrapes = 250\n",
    "\n",
    "###\n",
    "# Looping through each post requires two requests.\n",
    "###\n",
    "for i in range(n_scrapes):\n",
    "    j = i % 25\n",
    "    this_dict = {}\n",
    "    url_bpt = url_base + slug_bpt + slug_hot + aft\n",
    "    res = requests.get(url_bpt, headers = user)   # First request: get information about the post\n",
    "    data = res.json()\n",
    "\n",
    "    slug_bpt_id = data['data']['children'][j]['data']['id']\n",
    "    comments_this_post = data['data']['children'][j]['data']['num_comments']\n",
    "    post_title = data['data']['children'][j]['data']['title']\n",
    "    \n",
    "    if (i+1)%25==0: # Gives an update every 25 posts, and moves to the next page (using the aft variable).\n",
    "        print(\"{} posts scraped!\".format(i+1))\n",
    "        aft = '?after='+data['data']['after']\n",
    "    \n",
    "    this_dict['comments_this_post'] = comments_this_post\n",
    "    this_dict['post_title'] = post_title\n",
    "\n",
    "    # Now that the post info is added, we move on to the comment\n",
    "    \n",
    "    url_bpt_comments = url_base + slug_bpt + 'comments/' + slug_bpt_id + '.json'\n",
    "    res = requests.get(url_bpt_comments, headers = user) # Second request: get information about the top comment.\n",
    "    data = res.json()\n",
    "    try:\n",
    "        comment_data = data[1]['data']['children'][0]['data'] # If there isn't a comment, pass.\n",
    "    except:\n",
    "        pass\n",
    "    for feature in features:\n",
    "        this_dict[feature] = comment_data[feature] # add to the dictionary\n",
    "    this_dict['current_time'] = time.time() # an extra column for the time the post was scraped.\n",
    "    time.sleep(2) # Keeping us from getting booted out.\n",
    "    list_of_dictionaries.append(this_dict)  \n",
    "\n",
    "###\n",
    "# Finally, we save the file with the current month, day, and hour\n",
    "###\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "pd.DataFrame(list_of_dictionaries).to_csv('Bpt_250_{}-{}_{}{}.csv'.format(now.month, now.day, now.hour, now.minute))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The same for r/WhitePeopleTwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dictionaries = []\n",
    "aft = ''\n",
    "features = ['created_utc', 'body', 'ups']\n",
    "n_scrapes = 250\n",
    "\n",
    "###\n",
    "# Looping through each post requires two requests.\n",
    "###\n",
    "for i in range(n_scrapes):\n",
    "    j = i % 25\n",
    "    this_dict = {}\n",
    "    url_wpt = url_base + slug_wpt + slug_hot + aft\n",
    "    res = requests.get(url_wpt, headers = user)   # First request: get information about the post\n",
    "    data = res.json()\n",
    "\n",
    "    slug_wpt_id = data['data']['children'][j]['data']['id']\n",
    "    comments_this_post = data['data']['children'][j]['data']['num_comments']\n",
    "    post_title = data['data']['children'][j]['data']['title']\n",
    "    \n",
    "    if (i+1)%25==0: # Gives an update every 25 posts, and moves to the next page (using the aft variable).\n",
    "        print(\"{} posts scraped!\".format(i+1))\n",
    "        aft = '?after='+data['data']['after']\n",
    "    \n",
    "    this_dict['comments_this_post'] = comments_this_post\n",
    "    this_dict['post_title'] = post_title\n",
    "\n",
    "    # Now that the post info is added, we move on to the comment\n",
    "    \n",
    "    url_wpt_comments = url_base + slug_wpt + 'comments/' + slug_wpt_id + '.json'\n",
    "    res = requests.get(url_wpt_comments, headers = user) # Second request: get information about the top comment.\n",
    "    data = res.json()\n",
    "    try:\n",
    "        comment_data = data[1]['data']['children'][0]['data'] # If there isn't a comment, pass.\n",
    "    except:\n",
    "        pass\n",
    "    for feature in features:\n",
    "        this_dict[feature] = comment_data[feature] # add to the dictionary\n",
    "    this_dict['current_time'] = time.time() # an extra column for the time the post was scraped.\n",
    "    time.sleep(2) # Keeping us from getting booted out.\n",
    "    list_of_dictionaries.append(this_dict)  \n",
    "\n",
    "###\n",
    "# Finally, we save the file with the current month, day, and hour\n",
    "###\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "pd.DataFrame(list_of_dictionaries).to_csv('Wpt_250_{}-{}_{}{}.csv'.format(now.month, now.day, now.hour, now.minute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
